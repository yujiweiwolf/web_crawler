#!/usr/bin/python3
import requests
from bs4 import BeautifulSoup
import pdfplumber
import chardet
import pandas as pd
import os
import sys
import json
from collections import deque
import re
from datetime import datetime, timedelta

class Employee:
    def __init__(self, name, code, date, url):
        self.name = name
        self.code = code
        self.date = date
        self.url = url

def traverse_json(data, my_list):
    queue = deque([(data, '')])
    name = ""
    code = ""
    date = ""
    url = ""
    while queue:
        value, prefix = queue.popleft()
        if isinstance(value, dict):
            for key, val in value.items():
                queue.append((val, prefix + f'["{key}"]'))
        elif isinstance(value, list):
            for i, val in enumerate(value):
                queue.append((val, prefix + f'[{i}]'))
        else:
            # print(prefix, ':', value)
            if (prefix.find("data")!= -1):
                if (prefix.find("SECURITY_CODE")!= -1):
                    code = value + ".SH"
                if (prefix.find("SECURITY_NAME")!= -1):
                    name = value
                if (prefix.find("SSEDATE")!= -1):
                    date = value
                if (prefix.find("URL")!= -1):
                    url = value
                    empl = Employee(name, code, date, url)
                    my_list.append(empl)

def write_csv_from_url(url, code, name,fo):
    print(url)

    response = requests.get(url)
    if response.status_code == 200:
        print("请求成功！")

        with open('downloaded_pdf.pdf', 'wb') as f:
            f.write(response.content)
            f.close()

        pdf =  pdfplumber.open(file_name)
        num_pages = len(pdf.pages)
        print(f"Number of pages: {num_pages}")


        for page in pdf.pages:
            table = page.extract_table()
            if table:
                index = 0
                for row in table:
                    index = index + 1
                    if (len(row) == 10) :
                        # print(row)
                        if (index == 1) :                        
                            # print ("第一行的长度 %d" % len(row))
                            if ((str(row[1]) == "投资者名称")& (str(row[2]) == "配售对象名称")):
                                flag = True                            
                        else :
                            if (flag & (row[len(row) - 1] == "A")) :
                                accout = str(row[2]).replace(",", "").replace("\n", "")
                                volume = str(row[5]).replace(",", "").replace("\n", "")
                                amount = str(row[8]).replace(",", "").replace("\n", "")
                                result = code + "," + name + "," + accout + "," + volume + "," + amount
                                fo.write(str(result).replace(" ", "").replace("\n", "") + "\n")                    
                    else :
                        flag = False


os.system('clear')

today = datetime.now()
pre_day = today + timedelta(days=-60)
str_today = datetime.now().strftime('%Y-%m-%d')
str_pre_date = pre_day.strftime('%Y-%m-%d')


sh_Referer = "http://www.sse.com.cn"
url = 'http://query.sse.com.cn/security/stock/queryCompanyBulletinNew.do'
params = {
    "BULLETIN_TYPE": "",
    "START_DATE": str_pre_date,
    "END_DATE": str_today,
    # "START_DATE": "2024-11-24",
    # "END_DATE": "2025-02-24",
    "SECURITY_CODE": "",
    "TITLE": "网下初步配售结果及网上中签结果",
    "beginDate": "",
    "endDate": "",
    "isNew": False,
    "isPagination": "true",
    "jsonCallBack": "jsonpCallback75011929",
    "keyWord": "",
    "stockType": "",
    "pageHelp.pageSize": 25,
    "pageHelp.cacheSize": 1,
    "pageHelp.pageNo": 1,
    "pageHelp.beginPage": 1,
    "pageHelp.endPage": 1
}
headers = {
    "Host": "query.sse.com.cn",
    "Referer": sh_Referer,
    "Cookie": "gdp_user_id=gioenc-966e7g6g%2Cg38c%2C5gcg%2C9350%2C9g7471462e46; JSESSIONID=5D7B8A09E1982A04E8CBF17B66D6A89E; ba17301551dcbaf9_gdp_session_id=06e0a191-8f18-48c8-86ac-f801799597fa; ba17301551dcbaf9_gdp_session_id_sent=06e0a191-8f18-48c8-86ac-f801799597fa; ba17301551dcbaf9_gdp_sequence_ids={%22globalKey%22:629%2C%22VISIT%22:8%2C%22PAGE%22:21%2C%22VIEW_CLICK%22:602%2C%22CUSTOM%22:2}",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
}

file_name = 'downloaded_pdf.pdf'
save_name = '新股网下发行获配明细表.csv'
#save_name = 'aaa.csv'
my_dict = {}


exit_flag = os.path.exists(save_name)
print(exit_flag)
if (exit_flag):
    with open(save_name, 'r') as file:
        lines = file.readlines()
        for line in lines:
            result = line[0:4]
            if (result != "code") :
                my_dict[line[0:9]] = 1
    print(my_dict)

fo = open(save_name, "a+") 
if (exit_flag == False):
    fo.write("code,name,配售对象名称,初步配售数量（股), 获配金额（元)\n")

r = requests.get(url=url, params=params, headers=headers)
print(r.status_code)
if r.status_code == 200:
    print("请求成功！")
    result = r.text
    start = result.find("({")
    json_data = r.text[start + 1 : len(result) - 1]
    print("\n")
    format_data = json.loads(json_data)
    my_list = []
    traverse_json(format_data, my_list)
    for value in my_list:
        print(value.name, value.code, value.date, sh_Referer + value.url)
        if value.code in my_dict:
            print("Key %s exists in the dictionary." % value.code)
        else:
            print("Key %s does not exist in the dictionary."% value.code)
            write_csv_from_url(sh_Referer + value.url, value.code, value.name, fo)
        #break
fo.close()
